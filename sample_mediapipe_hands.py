# mediapipe Hands (python) ã¨MMMã§è»¢ç§»å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã‚¸ã‚§ã‚¹ãƒãƒ£èªè­˜ã™ã‚‹
# ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œã‚µãƒ³ãƒ—ãƒ«: https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer/python

import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from printer import controller as printer
import numpy as np
import copy
import argparse

import cv2 as cv

model_path = './model/gesture_recognizer.task'

BaseOptions = mp.tasks.BaseOptions
GestureRecognizer = mp.tasks.vision.GestureRecognizer
GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions
GestureRecognizerResult = mp.tasks.vision.GestureRecognizerResult
VisionRunningMode = mp.tasks.vision.RunningMode

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--device", type=int, default=0)
    parser.add_argument("--width", help='cap width', type=int, default=960)
    parser.add_argument("--height", help='cap height', type=int, default=540)
    parser.add_argument('--use_static_image_mode', action='store_true')
    parser.add_argument("--min_detection_confidence",
                        help='min_detection_confidence',
                        type=float,
                        default=0.7)
    parser.add_argument("--min_tracking_confidence",
                        help='min_tracking_confidence',
                        type=int,
                        default=0.5)

    args = parser.parse_args()
    return args

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¸keypointsæç”»ã—ãŸç”»åƒã‚’é€ã‚Šè¿”ã™
def play_processed_image(output_image):

    print(f'-------playing dummy video-------')


    # imgencode=cv.imencode('.jpg',output_image)[1]
    # stringData=imgencode.tostring()
    # yield (b'--frame\r\n'
    #     b'Content-Type: text/plain\r\n\r\n'+stringData+b'\r\n')
    
    # ã‚‚ã—ç”»ç´ ãŒåè»¢ã—ã¦ã„ãŸã‚‰        
    # image_ = cv.cvtColor(image_, cv.COLOR_BGR2RGB)
    # recognition_result = recognizer_.recognize(image_)

# æ¤œå‡ºã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
# Create a hand landmarker instance with the live stream mode:
def print_result(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):
    print('gesture recognition result: {}'.format(result))

    # ã“ã®ç”»åƒã‚’ç”»é¢ã«è¡¨ç¤ºã™ã‚‹
    play_processed_image(output_image)

    # # å°åˆ·æŒ‡ç¤º
    # if not isPrinting_:
    #     isPrinting_ = True
    #     ### start thread (escposãŒasyncioæœªå¯¾å¿œã®ãŸã‚concurrent.futureã§ãƒ©ãƒƒãƒ—ã—ã¦ã„ã¾ã™ğŸ˜¿) ###
    #     # ref. https://gist.github.com/tag1216/40b75346fd4ffdbfba22a55905094b0e#file-03_map-py
    #     with ThreadPoolExecutor(max_workers=2, thread_name_prefix="print_thread") as executor:
    #         future = executor.submit(printer.output_and_cut, "XXXXXXX!")
    #     # print(future.result())
    # ### end thread ###
    # isPrinting_ = False

    ### ã‚¸ã‚§ã‚¹ãƒãƒ£ã®æˆ»ã‚Šå€¤å½¢å¼
    # GestureRecognizerResult:
    #   Handedness:
    #     Categories #0:
    #       index        : 0
    #       score        : 0.98396
    #       categoryName : Left
    #   Gestures:
    #     Categories #0:
    #       score        : 0.76893
    #       categoryName : Thumb_Up
    #   Landmarks:
    #     Landmark #0:
    #       x            : 0.638852
    #       y            : 0.671197
    #       z            : -3.41E-7
    #     Landmark #1:
    #      ...


# åˆæœŸåŒ–
def prepare():
    # printeræº–å‚™
    # printer.init_printer()
    
    # å¼•æ•°è§£æ
    args = get_args()
    cap_device = args.device
    cap_width = args.width
    cap_height = args.height

    # use_static_image_mode = args.use_static_image_mode
    # min_detection_confidence = args.min_detection_confidence
    # min_tracking_confidence = args.min_tracking_confidence
    # use_brect = True

    # ã‚«ãƒ¡ãƒ©æº–å‚™ 
    # Use OpenCVâ€™s VideoCapture to start capturing from the webcam.
    cap=cv.VideoCapture(cap_device)
    cap.set(cv.CAP_PROP_FRAME_WIDTH, cap_width)
    cap.set(cv.CAP_PROP_FRAME_HEIGHT, cap_height)

    # Create an GestureRecognizer object.
    options = GestureRecognizerOptions(
        base_options=BaseOptions(model_asset_path=model_path),
        running_mode=VisionRunningMode.LIVE_STREAM,
        result_callback=print_result)

    return (cap, options)

def execute():

    # mp_hands = mp.solutions.hands
    # mp_drawing = mp.solutions.drawing_utils
    # mp_drawing_styles = mp.solutions.drawing_styles

    counter_ = 0
    isPrinting_ = False
    image_ = None
    frames = []

    cap_,options = prepare()

    with GestureRecognizer.create_from_options(options) as recognizer:
        # The landmarker is initialized. Use it here.
        # ...
        # Create a loop to read the latest frame from the camera using VideoCapture#read()
        try:
            while True:
                # counter_+=1
                # if counter_==3:
                #     counter_ = 0
                #     continue

                # ã‚«ãƒ¡ãƒ©ã‚­ãƒ£ãƒ—ãƒãƒ£ #####################################################
                ret, image_ = cap_.read()
                ms_timestamp = cap_.get(cv.CAP_PROP_POS_MSEC)

                if not ret: return
                image_ = cv.flip(image_, 1)  # ãƒŸãƒ©ãƒ¼è¡¨ç¤º

                #debug_image = copy.deepcopy(image_)

                # Convert the frame received from OpenCV to a MediaPipeâ€™s Image object.
                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_) # data =  numpy_frame_from_opencv
                
                # æ¤œå‡ºå®Ÿæ–½ 
                # ç”»åƒã¾ãŸã¯ãƒ“ãƒ‡ã‚ªãƒ¢ãƒ‡ãƒ«ã§å®Ÿè¡Œã—ã¦ã„ã‚‹å ´åˆã€Image Embedder ã‚¿ã‚¹ã‚¯ã¯å…¥åŠ›ç”»åƒã¾ãŸã¯ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‡¦ç†ãŒå®Œäº†ã™ã‚‹ã¾ã§ç¾åœ¨ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ã¾ã™ã€‚
                # ã¨ã‚ã‚‹ã®ã§ã€livestreamãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™
                # TODO: HandLandmarkerOptionsã§è¨­å®šã—ãŸresult_callbackã«resultãŒå…¥ã£ã¦ã„ã‚‹
                
                # Send live image data to perform hand landmarks detection.
                # The hand landmarker must be created with the live stream mode.
                recognizer.recognize_async(mp_image, ms_timestamp)

                # çµæœã®å‡¦ç†ã¯ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¸

        except KeyboardInterrupt:
            print('===KeyboardInterrupt===')
        except Exception as e:
            print(f'===Exception===\n{e.with_traceback}')
        finally:
            cap_.release()
            del(cap_)
            cv.destroyAllWindows()
            return
        
if __name__ == "__main__":
    execute()
