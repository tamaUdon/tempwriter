# mediapipe Hands (python) ã¨MMMã§è»¢ç§»å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã‚¸ã‚§ã‚¹ãƒãƒ£èªè­˜ã™ã‚‹
# ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œã‚µãƒ³ãƒ—ãƒ«: https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer/python
# callbacké–¢æ•°ã®å‚è€ƒ: https://discuss.streamlit.io/t/unable-to-view-integrated-webcam-window/44153
# draw_landmarksã®å‚è€ƒ: https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb

# TODO: typingå°å…¥ã™ã‚‹

import mediapipe as mp
import functools
import threading
import numpy as np
import cv2 as cv
import asyncio

from asyncio import events
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from printer import controller as printer
from mediapipe.framework.formats import landmark_pb2
from typing import Any, Callable, TypeVar

T = TypeVar("T")

from concurrent.futures import ThreadPoolExecutor

model_path = './model/gesture_recognizer.task'

BaseOptions = mp.tasks.BaseOptions
GestureRecognizer = mp.tasks.vision.GestureRecognizer
GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions
GestureRecognizerResult = mp.tasks.vision.GestureRecognizerResult
VisionRunningMode = mp.tasks.vision.RunningMode

lock = threading.Lock()
current_gestures = []
current_landmarks = []
last_gesture = []
num_hands = 2

mp_drawing = mp.solutions.drawing_utils
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=num_hands,
    min_detection_confidence=0.65,
    min_tracking_confidence=0.65)

# ã‚¸ã‚§ã‚¹ãƒãƒ£èªè­˜ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
def __callback(result: GestureRecognizerResult, output_image: mp.Image, timestamp_ms: int):
    lock.acquire() # solves potential concurrency issues

    # ã‚¸ã‚§ã‚¹ãƒãƒ£èªè­˜ã•ã‚Œã¦ã„ãŸã‚‰ãƒ‘ãƒ¼ã‚¹ã—ã¦é…åˆ—ã«æ ¼ç´
    if result is not None and any(result.gestures):
        print("Recognized gestures:")
        for single_hand_gesture_data in result.gestures:
            gesture_name = single_hand_gesture_data[0].category_name
            print(gesture_name)
            current_gestures.append(gesture_name)

    # handèªè­˜ã•ã‚Œã¦ã„ãŸã‚‰ãƒ‘ãƒ¼ã‚¹ã—ã¦é…åˆ—ã«æ ¼ç´
    if not result is not None and any(result.hand_landmarks):
        for hand_landmarks in result.hand_landmarks:
            current_landmarks.append(hand_landmarks)
            
    # 300frameã«1å›print
    if timestamp_ms%60*5 == 0:
        printer.output_and_cut("appleğŸorangeğŸŠbanannağŸŒ")
                            
    lock.release()

    ### ã‚¸ã‚§ã‚¹ãƒãƒ£ã®æˆ»ã‚Šå€¤å½¢å¼
    # GestureRecognizerResult:
    #   Handedness:
    #     Categories #0:
    #       index        : 0
    #       score        : 0.98396
    #       categoryName : Left
    #   Gestures:
    #     Categories #0:
    #       score        : 0.76893
    #       categoryName : Thumb_Up
    #   Landmarks:
    #     Landmark #0:
    #       x            : 0.638852
    #       y            : 0.671197
    #       z            : -3.41E-7
    #     Landmark #1:
    #      ...

# ã‚¸ã‚§ã‚¹ãƒãƒ£ç¨®é¡ã‚’æç”»
def put_gestures(image):
    lock.acquire()
    gestures = current_gestures
    lock.release()
    y_pos = 50
    for hand_gesture_name in gestures:
    # show the prediction on the frame
        cv.putText(image, hand_gesture_name, 
                   (10, y_pos), 
                   cv.FONT_HERSHEY_SIMPLEX, 
                   1, (0,0,255), 2, 
                   cv.LINE_AA)
        y_pos += 50
    return image

# landmarkæç”»
def put_landmarks(image):
    lock.acquire()
    landmarks = current_landmarks
    lock.release()
    # mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image.numpy_view())

    # Draw the hand landmarks.
    mp_drawing.draw_landmarks(
      image,
      landmarks,
      mp.solutions.hands.HAND_CONNECTIONS,
      mp.solutions.drawing_styles.get_default_hand_landmarks_style(), # If this argument is explicitly set to None, no landmarks will be drawn.
      mp.solutions.drawing_styles.get_default_hand_connections_style()) # If this argument is explicitly set to None, no connections will be drawn.
    
# # ã‚¸ã‚§ã‚¹ãƒãƒ£ã‚’ç”»åƒã«ãƒãƒƒãƒ—
# def map_gesture_to_img():
#     # if current_gestures[0] == "a":
#     #     raise NotImplementedError()
#     # elif current_gestures[0] == "p":
#     #     raise NotImplementedError()
#     # elif current_gestures[0] == "l":
#     #     raise NotImplementedError()
#     # elif current_gestures[0] == "e":
#     #     raise NotImplementedError()
#     # elif current_gestures[0] == "none":
#     #     raise NotImplementedError()
#     print(f'current gestures[0]={current_gestures[0]}')
#     return "txttxttxttxttxttxt"

# # ä»Šå›èªè­˜ã—ãŸã‚¸ã‚§ã‚¹ãƒãƒ£ã¨åŒã˜ã‹ãƒã‚§ãƒƒã‚¯
# def is_gesture_changed():
#     if last_gesture[0] == current_gestures[0]:
#         return False
#     return True

# # å‰å›èªè­˜ã—ãŸã‚¸ã‚§ã‚¹ãƒãƒ£ã‚’è¨˜æ†¶
# def update_last_gesture():
#     lock.acquire()
#     last_gesture = current_gestures
#     current_gestures = []
#     current_landmarks = []
#     lock.release()

# åˆæœŸåŒ–
def prepare():
    # printeræº–å‚™
    printer.init_printer()
    
    # ã‚«ãƒ¡ãƒ©æº–å‚™ 
    # Use OpenCVâ€™s VideoCapture to start capturing from the webcam.
    cap=cv.VideoCapture(0)
    cap.set(cv.CAP_PROP_FRAME_WIDTH, 960)
    cap.set(cv.CAP_PROP_FRAME_HEIGHT, 540)

    # Create an GestureRecognizer object.
    options = GestureRecognizerOptions(
        base_options=BaseOptions(model_asset_path=model_path),
        running_mode=VisionRunningMode.LIVE_STREAM,
        result_callback=__callback)

    return (cap, options)

# ã‚¿ã‚¤ãƒ—ãƒ©ã‚¤ã‚¿ãƒ¼å®Ÿè¡Œ
def execute():
    ms_timestamp = 0
    image_ = None
    cap_,options = prepare()
    isPrinting = False

    with GestureRecognizer.create_from_options(options) as recognizer:
        try:
            while True:
                # ã‚«ãƒ¡ãƒ©ã‚­ãƒ£ãƒ—ãƒãƒ£ #####################################################
                ret, image_ = cap_.read()
                if not ret: break

                image_ = cv.flip(image_, 1)  # ãƒŸãƒ©ãƒ¼è¡¨ç¤º
                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_) # data =  numpy_frame_from_opencv
                recognizer.recognize_async(mp_image, ms_timestamp) # æ¤œå‡ºå®Ÿæ–½, çµæœã®å‡¦ç†ã¯ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¸
                    
                # landmarkã‚’æç”»ã—ã¾ã™
                copied_ = image_.copy()
                # im_ = put_gestures(copied_)
                put_landmarks(copied_)
                mp_drawing.draw_landmarks(
                    copied_,
                    current_landmarks,
                    mp.solutions.hands.HAND_CONNECTIONS,
                    mp.solutions.drawing_styles.get_default_hand_landmarks_style(), # If this argument is explicitly set to None, no landmarks will be drawn.
                    mp.solutions.drawing_styles.get_default_hand_connections_style()) # If this argument is explicitly set to None, no connections will be drawn.

                ms_timestamp += 1 # should be monotonically increasing, because in LIVE_STREAM mode
                # update_last_gesture()

                cv.imshow('MediaPipe Hands', copied_)
                if cv.waitKey(1) & 0xFF == 27:
                    break

        except KeyboardInterrupt:
            print('===KeyboardInterrupt===')
        except Exception as e:
            print(f'===Exception===\n{e}')
        finally:
            cap_.release()
            del(cap_)
            cv.destroyAllWindows()
            return
        
if __name__ == "__main__":
    execute()
